---
title: "Airbnb Pricing in Shanghai / China"
subtitle: "Data Analysis 3 - Assignment 2"
author: "Ersan Kucukoglu"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
```

```{r, include=FALSE}
# initialize packages -------------------------------------------------------------------
library(tidyverse)
library(skimr)
library(Hmisc)
library(cowplot)
library(data.table)
library(rattle)
library(caret)
library(ranger)
library(knitr)
library(kableExtra)
library(xtable)
library(rpart)
library(pdp)
library(scales)
library(ggpubr)

```

```{r, include=FALSE, cache=TRUE}
# Load clean data
data <- read_csv("~/Desktop/BA-Courses/Data_Analysis3/Assignment2/data/clean/shanghai_workfile.csv")

```

## Introduction
This study focuses on a single location - Shanghai, China - and seeks to estimate airbnb rental costs per night. The aim of the study was to assist company to predict prices for their small and mid-sized apartment accommodating 2-6 people using different prediction model. I'll be using data from Inside Airbnb to create these pricing prediction models. I'll be creating and comparing Airbnb predicting models for the city of **Shanghai**, China, to determine the optimal combination algorithms for assessing the prediction model. OLS Linear Regression, Lasso, and Random Forest were three machine learning techniques that I utilized.
The used dataset was downloaded from [insideairbnb.com](http://insideairbnb.com/get-the-data.html) which is a site collecting data on Airbnb listings in numerous cities. As mentioned above the dataset contains data on listings in Shanghai and was last updated on 24th December 2021.

### Feature engineering
Having decided about the functional form of the target variable I inspected the explanatory variables and their relationship with the target variable. Besides deciding on grouping of factor variables functional forms also had to be decided because two of the used prediction models were OLS and OLS with LASSO for which this step is necessary.

```{r echo=FALSE, message=FALSE, warning=FALSE,fig.width=8,fig.height=3}
# boxplot of price by property type
ggplot(data = data, aes(x = f_property_type, y = price)) +
  stat_boxplot(aes(group = f_property_type), geom = "errorbar", width = 0.3, size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = f_property_type),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA)+
labs(x = "Property type",y = "Price")
```
First, I inspected the two factor variables: property type and neighbourhood. The prices conditional on property type can be seen in the chart above. Since there are differences between the three categories in conditional means as well as standard deviation I decided to keep all three.

I examined the dummies after the categorical variables. I calculated the conditional mean price for each dummy and came to the conclusion that my assumptions were correct in the sense that an apartment with more extras priced more in general.
I also checked the number of missing values for each of the variables.  As for the explanatory variables :  I imputed missing values using the variable's median according to some methods. Lastly, I examined possible interactions between the property type factor variable and all the dummy variables by using plots. I created two lists of interactions: one for OLS and one for LASSO.
```{r, include=FALSE, cache=TRUE}
# check missing values ----------------------------------------------------

to_filter <- sapply(data, function(x) sum(is.na(x)))
to_filter[to_filter > 0]

# impute without flags where there are only a few missing
data <- data %>% 
  mutate( n_host_total_listings_count = ifelse(is.na(n_host_total_listings_count), median(n_host_total_listings_count, na.rm = T), n_host_total_listings_count),
          ln_bathrooms = ifelse(is.na(ln_bathrooms), 1, ln_bathrooms),
          n_bedrooms = ifelse(is.na(n_bedrooms),n_beds%/%2, n_bedrooms)) # assume that there are two beds in a bedroom 

# redo their polinomials and logs
data <- data %>% 
  mutate(ln_host_total_listings_count = log(n_host_total_listings_count+1),
         n_bedrooms2 = n_bedrooms^2)

# impute with flags where there are more missing
data <- data %>%
  mutate(
    flag_days_since_rv=ifelse(is.na(n_days_since_rv),1, 0),
    n_days_since_rv =  ifelse(is.na(n_days_since_rv), median(n_days_since_rv, na.rm = T), n_days_since_rv),
    flag_review_scores_rating=ifelse(is.na(n_review_scores_rating),1, 0),
    n_review_scores_rating =  ifelse(is.na(n_review_scores_rating), median(n_review_scores_rating, na.rm = T), n_review_scores_rating),
    flag_reviews_per_month=ifelse(is.na(n_reviews_per_month),1, 0),
    n_reviews_per_month =  ifelse(is.na(n_reviews_per_month), median(n_reviews_per_month, na.rm = T), n_reviews_per_month),
    flag_number_of_reviews=ifelse(n_number_of_reviews==0,1, 0),
    flag_host_response_rate = ifelse(is.na(n_host_response_rate), 1, 0),
    n_host_response_rate = ifelse(is.na(n_host_response_rate), median(n_host_response_rate, na.rm = T), n_host_response_rate),
    flag_host_acceptance_rate = ifelse(is.na(n_host_acceptance_rate), 1, 0),
    n_host_acceptance_rate = ifelse(is.na(n_host_acceptance_rate), median(n_host_acceptance_rate, na.rm = T), n_host_acceptance_rate)
  )

# redo their polinomials and logs
data <- data %>% mutate(
  n_days_since_rv2 = n_days_since_rv^2,
  n_host_acceptance_rate2 = n_host_acceptance_rate^2,
  n_host_response_rate2 = n_host_response_rate^2
)

```


```{r, include=FALSE, cache=TRUE}
# preparation for modeling ------------------------------------------------

# group variables
target_var <- 'price'

basic_vars <- c('f_property_type', 'f_neighbourhood', 'n_accommodates', 'n_bedrooms', 'n_beds', 'd_instant_bookable', 'n_bathrooms')

host_vars <- c( 'n_host_response_rate', 'flag_host_response_rate','n_host_acceptance_rate', 'flag_host_acceptance_rate', 'd_superhost', 'd_profile_pic', 'd_identity_verified', 'n_host_total_listings_count')

reviews <- c(  'n_number_of_reviews', 'flag_number_of_reviews', 'n_days_since_rv', 'flag_days_since_rv', 'n_review_scores_rating', 'flag_review_scores_rating','n_reviews_per_month', 'flag_reviews_per_month')

amenities <- c('d_wifi', 'd_tv', 'd_refrigerator', 'd_air_conditioning', 'd_microwave', 'd_baby', 'd_stove', 'd_free_parking', 'd_paid_parking')

transformed_vars <- c( 'n_beds2', 'ln_bathrooms', 'ln_number_of_reviews', 'n_days_since_rv2', 'n_host_acceptance_rate2', 'n_host_response_rate2', 'ln_host_total_listings_count', 'n_bedrooms2')

X_for_ols <- c('f_property_type * d_profile_pic', 'f_property_type * d_instant_bookable', 'f_property_type * d_wifi', 'f_property_type * d_tv', 'f_property_type * d_refrigerator', 'f_property_type * d_baby', 'f_property_type * d_microwave', 'f_property_type * d_stove', 'f_property_type * d_paid_parking')

X_for_lasso  <- c('f_property_type * d_profile_pic', 'f_property_type * d_identity_verified', 'f_property_type * d_superhost', 'f_property_type * d_instant_bookable', paste0("(f_property_type) * (",                                                                                                                                                                              paste(amenities, collapse=" + "),")"))

# group predictors for models

predictors_1 <- c(basic_vars)
predictors_2 <- c(basic_vars, host_vars, reviews, amenities)
predictors_3 <- c(basic_vars, host_vars, reviews, amenities, transformed_vars)
predictors_4 <- c(predictors_3, X_for_ols)
predictors_5 <- c(predictors_3, X_for_lasso)

# create holdout set
set.seed(890)

train_indices <- as.integer(createDataPartition(data$price, p = 0.8, list = FALSE))
df_train <- data[train_indices, ]
df_holdout <- data[-train_indices, ]

# set the number of folds for cross-validation
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = FALSE)
```



```{r, include=FALSE, cache=TRUE}
# helper function ----------------------------------------------------------
price_diff_by_variables2 <- function(df, factor_var, dummy_var, factor_lab, dummy_lab){
  # Looking for interactions.
  # It is a function it takes 3 arguments: 1) Your dataframe,
  # 2) the factor variable (like room_type)
  # 3)the dummy variable you are interested in (like TV)

  # Process your data frame and make a new dataframe which contains the stats
  factor_var <- as.name(factor_var)
  dummy_var <- as.name(dummy_var)

  stats <- df %>%
    group_by(!!factor_var, !!dummy_var) %>%
    dplyr::summarize(Mean = mean(price, na.rm=TRUE),
                     se = sd(price)/sqrt(n()))

  stats[,2] <- lapply(stats[,2], factor)

  ggplot(stats, aes_string(colnames(stats)[1], colnames(stats)[3], fill = colnames(stats)[2]))+
    geom_bar(stat='identity', position = position_dodge(width=0.9), alpha=0.8)+
    geom_errorbar(aes(ymin=Mean-(1.96*se),ymax=Mean+(1.96*se)),
                  position=position_dodge(width = 0.9), width = 0.25)+
    ylab('Mean Price')+
    xlab(factor_lab) +
    theme(panel.grid.major=element_blank(),
          panel.grid.minor=element_blank(),
          panel.border=element_blank(),
          axis.line=element_line(),
          legend.position = "top",
          #legend.position = c(0.7, 0.9),
          legend.box = "vertical",
          legend.text = element_text(size = 5),
          legend.title = element_text(size = 5, face = "bold"),
          legend.key.size = unit(x = 0.4, units = "cm")
        )
}

# check for interactions --------------------------------------------------

# check property type interactions
p1 <- price_diff_by_variables2(data, "f_property_type", "d_superhost", "Property Type", "Superhost")
p2 <- price_diff_by_variables2(data, "f_property_type", "d_profile_pic", "Property Type", "Profile Picture") 
p3 <- price_diff_by_variables2(data, "f_property_type", "d_identity_verified", "Property Type", "Identity Verified")
p4 <- price_diff_by_variables2(data, "f_property_type", "d_instant_bookable" , "Property Type", "Instant Bookable") 
p5 <- price_diff_by_variables2(data, "f_property_type", "d_wifi" , "Property Type", "Wifi") 
p6 <- price_diff_by_variables2(data, "f_property_type", "d_tv" , "Property Type", "Tv") 
p7 <- price_diff_by_variables2(data, "f_property_type", "d_refrigerator", "Property Type", "Refrigerator") 
p8 <- price_diff_by_variables2(data, "f_property_type", "d_air_conditioning" , "Property Type", "Air Conditioning")
p9 <- price_diff_by_variables2(data, "f_property_type", "d_microwave", "Property Type", "Microwave")
p10 <- price_diff_by_variables2(data, "f_property_type", "d_baby", "Property Type", "Baby Friendly")
p11 <- price_diff_by_variables2(data, "f_property_type", "d_stove", "Property Type", "Stove") 
p12 <- price_diff_by_variables2(data, "f_property_type", "d_free_parking", "Property Type", "Free Parking")
p13 <- price_diff_by_variables2(data, "f_property_type", "d_paid_parking", "Property Type", "Paid Parking") 

```

```{r, include = FALSE, cache=TRUE}
amenities <- c( "d_wifi", "d_tv", "d_refrigerator", "d_air_conditioning", "d_microwave", "d_baby", "d_stove", "d_free_parking", "d_paid_parking" )

# create lists of interactions
X_for_ols <- c('f_property_type * d_profile_pic', 'f_property_type * d_wifi','f_property_type * d_microwave', 'f_property_type * d_baby', 'f_property_type * d_stove', 'f_property_type * d_paid_parking')
X_for_lasso  <- c('f_property_type * d_superhost','f_property_type * d_identity_verified','f_property_type * d_profile_pic','f_property_type * d_air_conditioning','f_property_type * d_free_parking','f_property_type * d_refrigerator','f_property_type * d_tv', 'f_property_type * d_instant_bookable', paste0("(f_property_type) * (",paste(amenities, collapse=" + "),")"))
```

### OLS and LASSO

In total I estimated 3 OLS models with different sets of variables. The first one contained only the levels of variables, the second one contained the levels and the transformed versions, while the last one contained interactions as well. For LASSO, I estimated two models: one with levels and transformed versions of variables plus a few interactions and one with levels and transformed versions of variables plus all the interactions I previously determined.

```{r, include=FALSE, cache=TRUE}
# simplest model
set.seed(8)
system.time({
  ols_model1 <- train(
    formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
    data = df_train,
    method = "lm",
    trControl = train_control
  )
})

ols_model_coeffs1 <-  ols_model1$finalModel$coefficients
ols_model_coeffs_df1 <- data.frame(
  "variable" = names(ols_model_coeffs1),
  "ols_coefficient" = ols_model_coeffs1
) %>%
  mutate(variable = gsub("`","",variable))

# model with transformed variables
set.seed(8)
system.time({
  ols_model2 <- train(
    formula(paste0("price ~", paste0(predictors_3, collapse = " + "))),
    data = df_train,
    method = "lm",
    trControl = train_control
  )
})

ols_model_coeffs2 <-  ols_model2$finalModel$coefficients
ols_model_coeffs_df2 <- data.frame(
  "variable" = names(ols_model_coeffs2),
  "ols_coefficient" = ols_model_coeffs2
) %>%
  mutate(variable = gsub("`","",variable))

# model with transformed variables plus interactions
set.seed(8)
system.time({
  ols_model3 <- train(
    formula(paste0("price ~", paste0(predictors_4, collapse = " + "))),
    data = df_train,
    method = "lm",
    trControl = train_control
  )
})

ols_model_coeffs3 <-  ols_model3$finalModel$coefficients
ols_model_coeffs_df3 <- data.frame(
  "variable" = names(ols_model_coeffs3),
  "ols_coefficient" = ols_model_coeffs3
) %>%
  mutate(variable = gsub("`","",variable))
```



```{r, include=FALSE, cache=TRUE}
# OLS with LASSO ----------------------------------------------------------

# transformed numeric variables, no interactions
set.seed(8)
system.time({
  lasso_model1 <- train(
    formula(paste0("price ~", paste0(predictors_4, collapse = " + "))),
    data = df_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 1, by = 0.05)),
    trControl = train_control
  )
})

print(lasso_model1$bestTune$lambda)

lasso_coeffs1 <- coef(
  lasso_model1$finalModel,
  lasso_model1$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(lasso_coefficient = `s1`) 

lasso_coeffs_non_null1 <- lasso_coeffs1[!lasso_coeffs1$lasso_coefficient == 0,]

print(nrow(lasso_coeffs_non_null1))

# transformed numeric variables plus interactions
set.seed(8)
system.time({
  lasso_model2 <- train(
    formula(paste0("price ~", paste0(predictors_5, collapse = " + "))),
    data = df_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 1, by = 0.05)),
    trControl = train_control
  )
})

print(lasso_model2$bestTune$lambda)

lasso_coeffs2 <- coef(
  lasso_model2$finalModel,
  lasso_model2$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(lasso_coefficient = `s1`) 

lasso_coeffs_non_null2 <- lasso_coeffs2[!lasso_coeffs2$lasso_coefficient == 0,]

print(nrow(lasso_coeffs_non_null2))
```

```{r, include=FALSE, cache=TRUE}
regression_coeffs <- merge(ols_model_coeffs_df3, lasso_coeffs_non_null2, by = "variable", all=TRUE)
names(regression_coeffs) <- c('Variable', 'OLS 3', 'LASSO 2')
```

The table below gives information about the models' performance as well as the number of coefficients they had. We can observe that the best performing OLS 3 and LASSO 2 models roughly have the same number of coefficients based on both cross-validated and holdout RMSE, but they do not totally overlap. The difference in their performance is tiny.

```{r, cache=TRUE}
temp_models <-
  list("OLS 1" = ols_model1,
       "OLS 2" = ols_model2,
       "OLS 3" = ols_model3,
       "LASSO 1 (few interactions)" = lasso_model1,
       "LASSO 2 (all interactions)" = lasso_model2)

result_temp <- resamples(temp_models) %>% summary()

# get test RMSE
result_rmse <- imap(temp_models, ~{
  mean(result_temp$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")

# get holdout RMSE
result_holdout <- map(temp_models, ~{
  RMSE(predict(.x, newdata = df_holdout), df_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")

# merge the two
result_combined <- cbind(result_rmse, result_holdout )

# calculate number of variables in each model
num_coefs <-  c(
  length(ols_model1$coefnames),
  length(ols_model2$coefnames),
  length(ols_model3$coefnames),
  nrow(lasso_coeffs_non_null1),
  nrow(lasso_coeffs_non_null2))

ncoefs <- as.data.frame(num_coefs, row.names = rownames(result_combined)
) %>% rename("Number of Coefficients" = "num_coefs")

# merge the three
result_combined <- cbind(ncoefs, result_rmse, result_holdout )

# print table
knitr::kable( result_combined, caption = "OLS and LASSO performance", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position' )
```

Because the LASSO 2 model performed the best, I created the plot below, which displays the actual prices as well as the prices predicted by this model. We can observe that it gives better predictions for cheaper prices and somewhat underestimates prices.

```{r echo=FALSE, message=FALSE,warning=FALSE,show.fig="hold",fig.width=8,fig.height=3,fig.pos="centre"}
# fitted vs actual values for LASSO 2
# target variable
Ylev <- df_holdout[["price"]]

# get predicted values
predictionlev_holdout_pred <- as.data.frame(predict(lasso_model2, newdata = df_holdout))

# rename column
names(predictionlev_holdout_pred) <- "fit"

# Create data frame with the real and predicted values
d <- data.frame(ylev=Ylev, predlev=predictionlev_holdout_pred[,"fit"] )
# Check the differences
d$elev <- d$ylev - d$predlev

# Plot predicted vs price
level_vs_pred <- ggplot(data = d) +
  geom_point(aes(y=ylev, x=predlev),  size = 1,
             shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE, color = "blue") +
  geom_segment(aes(x = 0, y = 0, xend = 1300, yend =1100), size=1, linetype=2, color = "red") +
  coord_cartesian(xlim = c(0, 1100), ylim = c(0, 1300)) +
  labs(y = "Price", x = "Predicted price", title = "Actual vs fitted values for the LASSO 2 model") +
  theme_bw() +
  theme( panel.grid.minor.x = element_blank(), 
         plot.title = element_text( size = 12, face = "bold", hjust = 0.5 ) )
level_vs_pred
```

### Random forest

I estimated two random forest models in total using the levels of all variables. The models differ in the parameter sets that determine the number of randomly chosen variables at each split and the minimum number of observations in the terminal nodes for each tree. Following the rule of thumb first I set the number of randomly chosen variables at each split to 6 which is around the square route of all variables. However, as the table below shows both final models produced better results when setting the parameter to a higher number. 

```{r, include=FALSE, cache=TRUE}
# simpler model
tune_grid <- expand.grid(
  .mtry = c(6, 8, 10),
  .splitrule = "variance",
  .min.node.size = c(5, 10)
)

set.seed(8)
system.time({
  rf_model_1 <- train(
    formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
    data = df_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity"
  )
})

# more complex model
tune_grid <- expand.grid(
  .mtry = c(8, 10, 12),
  .splitrule = "variance",
  .min.node.size = c(5, 10, 15)
)

set.seed(8)
system.time({
  rf_model_2 <- train(
    formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
    data = df_train,
    method = "ranger",
    trControl = train_control,
    tuneGrid = tune_grid,
    importance = "impurity"
  )
})
```

```{r, include=FALSE, cache=TRUE}
# tuning parameter choice 1
result_1 <- matrix(c(
  rf_model_1$finalModel$mtry,
  rf_model_2$finalModel$mtry,
  rf_model_1$finalModel$min.node.size,
  rf_model_2$finalModel$min.node.size
),
nrow=2, ncol=2,
dimnames = list(c("Random forest 1", "Random forest 2"),
                c("Min. number of variables","Min. node size"))
)
# print table
knitr::kable( result_1, caption = "Best hyperparameter sets for random forest models", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position' )
```


```{r,include=FALSE, cache=TRUE}
# save results
results <- resamples(
  list(
    model_1  = rf_model_1,
    model_2  = rf_model_2
  ))

# summary table on model 2 for different hyperparameters
rf_tuning_model2 <- rf_model_2$results %>%
  dplyr::select(mtry, min.node.size, RMSE) %>%
  dplyr::rename(Nodes = min.node.size) %>%
  spread(key = mtry, value = RMSE)

# print table
knitr::kable( rf_tuning_model2, caption = "Different hyperparameter sets Random forest 2", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position' )
```

To be able to decide which model is better I calculated the cross-validated RMSE-s on the test sets. We can see in the table below that Random forest 2 performs better than Random forest 1. But again just like in the case of the OLS and LASSO models the difference between the two models is relatively small.

```{r, cache=TRUE}
# RMSE of models
result_2 <- matrix(c(mean(results$values$`model_1~RMSE`),
                     mean(results$values$`model_2~RMSE`)
),
nrow=2, ncol=1,
dimnames = list(c("Random forest 1", "Random forest 2"),
                c(results$metrics[2]))
)

names(result_2) <- "CV RMSE"

# print table
knitr::kable( result_2, caption = "Performance of random forest models", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position')
```

It is useful to see which factors contributed the most to the model's RMSE decrease.I created a variable significance plot for that model, which displayed the top ten variables that contributed the most to the reduction of RMSE in percentages. The very first variable indicates how many listings a host has in total.I was expecting the neighbourhood Huangbhu district in the first 3 variables, because it is heart of the city and popular.

```{r, include=FALSE, cache=TRUE}
# variable importance plot for random forest 2
rf_model_2_var_imp <- importance(rf_model_2$finalModel)/1000
rf_model_2_var_imp_df <-
  data.frame(varname = names(rf_model_2_var_imp),imp = rf_model_2_var_imp) %>%
  mutate(varname = gsub("f_neighbourhood", "Neighbourhood:", varname) ) %>%
  mutate(varname = gsub("f_room_type", "Room type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))

rf_model_2_var_imp_plot_b <- ggplot(rf_model_2_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color="red", size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color="blue", size=1) +
  ylab("Importance") +
  xlab("Variable Name") +
  labs( title= "Variable importance for Random forest 2 (top 10 variables)") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw() +
  theme( panel.grid.minor.x = element_blank(), 
         plot.title = element_text( size = 12, face = "bold", hjust = 0.5 ) )
rf_model_2_var_imp_plot_b
```

```{r echo=FALSE, message=FALSE, warning=FALSE,fig.width=8,fig.height=4}
  rf_model_2_var_imp_plot_b
```

### Conclusion
After running different types, I choose the best results from each method. I described their cross-validated RMSEs as well as their RMSEs determined on the holdout set in the table below. According to these data, the random forest model outperformed the LASSO and OLS models. However, the pricing plan I would recommend for the firm would be determined by their choices. If they want to see the relationship between specific explanatory factors and the target variable and have coefficients, I would recommend using either the LASSO model or the OLS because their performance is fairly close. Otherwise, I would suggest using the random forest model since that one has the best prediction performance.

```{r, cache=TRUE}
# Model selection ---------------------------------------------------------

final_models <-
  list("OLS 3" = ols_model3,
       "LASSO (all interactions)" = lasso_model2,
       "Random forest 2" = rf_model_2)

results <- resamples(final_models) %>% summary()

# evaluate final models on holdout set
final_rmse <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")

final_holdout <- map(final_models, ~{
  RMSE(predict(.x, newdata = df_holdout), df_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")

final_combined <- cbind(final_rmse, final_holdout)

# print table
knitr::kable( final_combined, caption = "Model performance comparison", digits = 2 ) %>% kable_styling( position = "center", latex_options = 'hold_position')
```


